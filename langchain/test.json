{'conversation': ConversationChain(memory=ConversationSummaryBufferMemory(human_prefix='Human', ai_prefix='AI', llm=ChatOpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.0, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None, tiktoken_model_name=None), prompt=PromptTemplate(input_variables=['summary', 'new_lines'
    ], output_parser=None, partial_variables={}, template='Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\n\nEXAMPLE\nCurrent summary:\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\n\nNew lines of conversation:\nHuman: Why do you think artificial intelligence is a force for good?\nAI: Because artificial intelligence will help humans reach their full potential.\n\nNew summary:\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\nEND OF EXAMPLE\n\nCurrent summary:\n{summary
    }\n\nNew lines of conversation:\n{new_lines
    }\n\nNew summary:', template_format='f-string', validate_template=True), summary_message_cls=<class 'langchain.schema.messages.SystemMessage'>, chat_memory=ChatMessageHistory(messages=[HumanMessage(content='tell me a joke about developers', additional_kwargs={}, example=False), AIMessage(content="Sure, here's a classic developer joke for you:\n\nWhy do programmers prefer dark mode?\n\nBecause light attracts bugs!", additional_kwargs={}, example=False)
    ]), output_key=None, input_key=None, return_messages=True, max_token_limit=3000, moving_summary_buffer='', memory_key='history'), callbacks=None, callback_manager=None, verbose=False, tags=None, metadata=None, prompt=ChatPromptTemplate(input_variables=['history', 'input'
    ], output_parser=None, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], output_parser=None, partial_variables={}, template="You are chatbot who receives alerts based on telemetry data from network devices. Your job is to make sense of the alert received and process for an engineer. Provide as  much help as possible, but don't invent. reply in markdown format", template_format='f-string', validate_template=True), additional_kwargs={}), MessagesPlaceholder(variable_name='history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'
        ], output_parser=None, partial_variables={}, template='{input
        }', template_format='f-string', validate_template=True), additional_kwargs={})
    ]), llm=ChatOpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.0, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None, tiktoken_model_name=None), output_key='response', output_parser=StrOutputParser(), return_final_only=True, llm_kwargs={}, input_key='input')
}
{'conversation': ConversationChain(memory=ConversationSummaryBufferMemory(human_prefix='Human', ai_prefix='AI', llm=ChatOpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.0, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None, tiktoken_model_name=None), prompt=PromptTemplate(input_variables=['summary', 'new_lines'
    ], output_parser=None, partial_variables={}, template='Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\n\nEXAMPLE\nCurrent summary:\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\n\nNew lines of conversation:\nHuman: Why do you think artificial intelligence is a force for good?\nAI: Because artificial intelligence will help humans reach their full potential.\n\nNew summary:\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\nEND OF EXAMPLE\n\nCurrent summary:\n{summary
    }\n\nNew lines of conversation:\n{new_lines
    }\n\nNew summary:', template_format='f-string', validate_template=True), summary_message_cls=<class 'langchain.schema.messages.SystemMessage'>, chat_memory=ChatMessageHistory(messages=[HumanMessage(content='tell me a joke about developers', additional_kwargs={}, example=False), AIMessage(content="Sure, here's a classic developer joke for you:\n\nWhy do programmers prefer dark mode?\n\nBecause light attracts bugs!", additional_kwargs={}, example=False), HumanMessage(content='now about lawyers', additional_kwargs={}, example=False), AIMessage(content="Certainly! Here's a lawyer joke for you:\n\nWhy don't lawyers go to the beach?\n\nBecause even the sand can't escape their cross-examination!", additional_kwargs={}, example=False)
    ]), output_key=None, input_key=None, return_messages=True, max_token_limit=3000, moving_summary_buffer='', memory_key='history'), callbacks=None, callback_manager=None, verbose=False, tags=None, metadata=None, prompt=ChatPromptTemplate(input_variables=['history', 'input'
    ], output_parser=None, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], output_parser=None, partial_variables={}, template="You are chatbot who receives alerts based on telemetry data from network devices. Your job is to make sense of the alert received and process for an engineer. Provide as  much help as possible, but don't invent. reply in markdown format", template_format='f-string', validate_template=True), additional_kwargs={}), MessagesPlaceholder(variable_name='history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'
        ], output_parser=None, partial_variables={}, template='{input
        }', template_format='f-string', validate_template=True), additional_kwargs={})
    ]), llm=ChatOpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.0, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None, tiktoken_model_name=None), output_key='response', output_parser=StrOutputParser(), return_final_only=True, llm_kwargs={}, input_key='input')
}
{'conversation': ConversationChain(memory=ConversationSummaryBufferMemory(human_prefix='Human', ai_prefix='AI', llm=ChatOpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.0, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None, tiktoken_model_name=None), prompt=PromptTemplate(input_variables=['summary', 'new_lines'
    ], output_parser=None, partial_variables={}, template='Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\n\nEXAMPLE\nCurrent summary:\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\n\nNew lines of conversation:\nHuman: Why do you think artificial intelligence is a force for good?\nAI: Because artificial intelligence will help humans reach their full potential.\n\nNew summary:\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\nEND OF EXAMPLE\n\nCurrent summary:\n{summary
    }\n\nNew lines of conversation:\n{new_lines
    }\n\nNew summary:', template_format='f-string', validate_template=True), summary_message_cls=<class 'langchain.schema.messages.SystemMessage'>, chat_memory=ChatMessageHistory(messages=[HumanMessage(content='tell me a joke about developers', additional_kwargs={}, example=False), AIMessage(content="Sure, here's a classic developer joke for you:\n\nWhy do programmers prefer dark mode?\n\nBecause light attracts bugs!", additional_kwargs={}, example=False), HumanMessage(content='now about lawyers', additional_kwargs={}, example=False), AIMessage(content="Certainly! Here's a lawyer joke for you:\n\nWhy don't lawyers go to the beach?\n\nBecause even the sand can't escape their cross-examination!", additional_kwargs={}, example=False), HumanMessage(content='now make a one, but consider the previous answer', additional_kwargs={}, example=False), AIMessage(content="Sure, here's a joke that combines both developers and lawyers:\n\nWhy did the developer become a lawyer?\n\nBecause they realized debugging code and cross-examining witnesses require the same level of attention to detail and finding the truth!", additional_kwargs={}, example=False)
    ]), output_key=None, input_key=None, return_messages=True, max_token_limit=3000, moving_summary_buffer='', memory_key='history'), callbacks=None, callback_manager=None, verbose=False, tags=None, metadata=None, prompt=ChatPromptTemplate(input_variables=['history', 'input'
    ], output_parser=None, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], output_parser=None, partial_variables={}, template="You are chatbot who receives alerts based on telemetry data from network devices. Your job is to make sense of the alert received and process for an engineer. Provide as  much help as possible, but don't invent. reply in markdown format", template_format='f-string', validate_template=True), additional_kwargs={}), MessagesPlaceholder(variable_name='history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'
        ], output_parser=None, partial_variables={}, template='{input
        }', template_format='f-string', validate_template=True), additional_kwargs={})
    ]), llm=ChatOpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.0, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None, tiktoken_model_name=None), output_key='response', output_parser=StrOutputParser(), return_final_only=True, llm_kwargs={}, input_key='input')
}
{'conversation': ConversationChain(memory=ConversationSummaryBufferMemory(human_prefix='Human', ai_prefix='AI', llm=ChatOpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.0, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None, tiktoken_model_name=None), prompt=PromptTemplate(input_variables=['summary', 'new_lines'
    ], output_parser=None, partial_variables={}, template='Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\n\nEXAMPLE\nCurrent summary:\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\n\nNew lines of conversation:\nHuman: Why do you think artificial intelligence is a force for good?\nAI: Because artificial intelligence will help humans reach their full potential.\n\nNew summary:\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\nEND OF EXAMPLE\n\nCurrent summary:\n{summary
    }\n\nNew lines of conversation:\n{new_lines
    }\n\nNew summary:', template_format='f-string', validate_template=True), summary_message_cls=<class 'langchain.schema.messages.SystemMessage'>, chat_memory=ChatMessageHistory(messages=[HumanMessage(content='tell me a joke about developers', additional_kwargs={}, example=False), AIMessage(content="Sure, here's a classic developer joke for you:\n\nWhy do programmers prefer dark mode?\n\nBecause light attracts bugs!", additional_kwargs={}, example=False), HumanMessage(content='now about lawyers', additional_kwargs={}, example=False), AIMessage(content="Certainly! Here's a lawyer joke for you:\n\nWhy don't lawyers go to the beach?\n\nBecause even the sand can't escape their cross-examination!", additional_kwargs={}, example=False), HumanMessage(content='now make a one, but consider the previous answer', additional_kwargs={}, example=False), AIMessage(content="Sure, here's a joke that combines both developers and lawyers:\n\nWhy did the developer become a lawyer?\n\nBecause they realized debugging code and cross-examining witnesses require the same level of attention to detail and finding the truth!", additional_kwargs={}, example=False), HumanMessage(content='can you do a summary of what we talked about?', additional_kwargs={}, example=False), AIMessage(content="Certainly! Here's a summary of what we discussed:\n\n- You asked for a joke about developers, and I shared a joke about why programmers prefer dark mode.\n- You then asked for a joke about lawyers, and I shared a joke about lawyers not going to the beach.\n- Finally, you requested a joke that combines both developers and lawyers, and I shared a joke about a developer becoming a lawyer due to the similarities in attention to detail and finding the truth.\n\nIs there anything else I can assist you with?", additional_kwargs={}, example=False)
    ]), output_key=None, input_key=None, return_messages=True, max_token_limit=3000, moving_summary_buffer='', memory_key='history'), callbacks=None, callback_manager=None, verbose=False, tags=None, metadata=None, prompt=ChatPromptTemplate(input_variables=['history', 'input'
    ], output_parser=None, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], output_parser=None, partial_variables={}, template="You are chatbot who receives alerts based on telemetry data from network devices. Your job is to make sense of the alert received and process for an engineer. Provide as  much help as possible, but don't invent. reply in markdown format", template_format='f-string', validate_template=True), additional_kwargs={}), MessagesPlaceholder(variable_name='history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'
        ], output_parser=None, partial_variables={}, template='{input
        }', template_format='f-string', validate_template=True), additional_kwargs={})
    ]), llm=ChatOpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.0, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None, tiktoken_model_name=None), output_key='response', output_parser=StrOutputParser(), return_final_only=True, llm_kwargs={}, input_key='input')
}